{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6 and 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 6 and 7 of O'Reilly's focuses on pipelines and using supervised learning classification algorithms with text-data. These chapters are the heart of the natural language processing and basic machine. In these examples we will introduce basic preprocessing steps such as the Count Vectorizer, scaling text data to assign a higher weight to rare words and a lower weight to common words, and how to detect context using ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import uniform, randint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses a binary classification algorithm to predict if a movie review is positive or negative. A label of 1 indicates a positive review and a label of 0 is a negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample data\n",
    "texts = [\n",
    "    \"I love this movie, it's amazing\",\n",
    "    \"This movie is terrible and boring\",\n",
    "    \"Great film, wonderful acting\",\n",
    "    \"Poor direction, bad acting, terrible movie\",\n",
    "    \"Excellent cinematography and great story\",\n",
    "    \"Waste of time, horrible movie\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline allows us to create handle the preprocessing and the model selection in a single object referred to as a pipeline. This is useful because we can run a grid search downstream to optimize both the preprocessing and the model hyperparameters to find the best combination of hyperparameters\n",
    "\n",
    "## Count Vectorizer\n",
    "\n",
    "### Short Text Classification\n",
    "\n",
    "\n",
    "+ Business Case: Customer service ticket categorization or email routing\n",
    "+ Justification: When dealing with short support tickets or emails, simple word presence/absence is often sufficient for categorization. The computational simplicity means faster processing of high-volume customer inquiries.\n",
    "+ Example: Automatically routing customer emails to different departments based on keywords\n",
    "\n",
    "\n",
    "### Keyword-Based Systems\n",
    "\n",
    "\n",
    "+ Business Case: Content tagging or basic document categorization\n",
    "+ Justification: When specific keywords strongly indicate category membership, counting occurrences is more interpretable for business stakeholders\n",
    "+ Example: Tagging product reviews with relevant product categories based on mentioned features\n",
    "\n",
    "\n",
    "### Resource-Constrained Environments\n",
    "\n",
    "\n",
    "+ Business Case: Real-time classification systems with limited computing resources\n",
    "+ Justification: Lower computational overhead compared to TF-IDF, making it more suitable for edge computing or mobile applications\n",
    "+ Example: Mobile app features that need to classify text with minimal battery impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Using CountVectorizer with Logistic Regression\n",
    "print(\"Using CountVectorizer with Logistic Regression:\")\n",
    "count_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(\n",
    "        stop_words='english'\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train and evaluate\n",
    "count_pipeline.fit(X_train, y_train)\n",
    "count_predictions = count_pipeline.predict(X_test)\n",
    "print(\"\\nCountVectorizer Results:\")\n",
    "print(classification_report(y_test, count_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to trim the noise by assigning a higher weight to rare words and a lower weight to common words instead of using stop words. The Term Freqency-Inverse Document Frequency (TF-IDF) scales text data to assign a higher probability to rare words and a lower probability to common words. \n",
    "\n",
    "Due to the formula, it is generally not necessary to remove stop words (unless there is a business justification) because common english stop words will already have a lower weight.\n",
    "\n",
    "Document Search and Retrieval\n",
    "\n",
    "\n",
    "+ Business Case: Enterprise search systems or content recommendation engines\n",
    "+ Justification: Better at identifying distinctive terms in documents, leading to more relevant search results\n",
    "+ Example: Internal document search system where finding the most relevant documents is crucial\n",
    "\n",
    "## TFIDF Vectorizer\n",
    "\n",
    "### Long-Form Content Analysis\n",
    "\n",
    "\n",
    "+ Business Case: Article categorization or research paper classification\n",
    "+ Justification: Accounts for term importance across the document corpus, reducing noise from commonly used words\n",
    "+ Example: Automatically categorizing news articles or research papers into topics\n",
    "\n",
    "\n",
    "### Competitive Intelligence\n",
    "\n",
    "\n",
    "+ Business Case: Analysis of competitor content or market research\n",
    "+ Justification: Better at identifying distinctive features in documents, helpful for understanding unique selling propositions\n",
    "+ Example: Analyzing competitor websites to identify key differentiating themes\n",
    "\n",
    "\n",
    "### Content Recommendation\n",
    "\n",
    "\n",
    "+ Business Case: Product description matching or content similarity analysis\n",
    "+ Justification: Better at capturing the importance of terms in context, leading to more nuanced recommendations\n",
    "+ Example: Suggesting similar products based on description similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Using TfidfVectorizer with Logistic Regression\n",
    "print(\"\\nUsing TfidfVectorizer with Logistic Regression:\")\n",
    "tfidf_pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        min_df=1,\n",
    "        max_df=0.8,\n",
    "        norm='l2'\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train and evaluate\n",
    "tfidf_pipeline.fit(X_train, y_train)\n",
    "tfidf_predictions = tfidf_pipeline.predict(X_test)\n",
    "print(\"\\nTfidfVectorizer Results:\")\n",
    "print(classification_report(y_test, tfidf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search \n",
    "\n",
    "### Pros:\n",
    "\n",
    "+ Systematically evaluates every possible parameter combination\n",
    "+ Guaranteed to find the optimal combination within the defined parameter space\n",
    "+ More reliable for smaller parameter spaces\n",
    "+ Results are reproducible and deterministic\n",
    "+ to interpret results due to systematic exploration\n",
    "\n",
    "### Cons:\n",
    "\n",
    "+ Computationally expensive, especially with large parameter spaces\n",
    "+ Time complexity increases exponentially with each additional parameter\n",
    "+ May waste resources exploring unproductive parameter combinations\n",
    "+ Not practical for high-dimensional parameter spaces\n",
    "+ Can be inefficient when some parameters are more important than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros:\n",
    "\n",
    "+ More efficient use of computational resources\n",
    "+ Can handle larger parameter spaces effectively\n",
    "+ Better at finding good parameters when some are more important than others\n",
    "+ Can explore continuous parameter distributions\n",
    "+ Usually finds good solutions much faster than grid search\n",
    "+ Allows more control over computational budget through n_iter parameter\n",
    "\n",
    "### Cons:\n",
    "\n",
    "+ May miss optimal parameter combinations due to random sampling\n",
    "+ Results can vary between runs due to randomness\n",
    "+ Less systematic, making it harder to ensure complete coverage of parameter space\n",
    "+ May require multiple runs to ensure stability of results\n",
    "+ Less suitable for small parameter spaces where exhaustive search is feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimizing Machine Learning Models\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Grid Search parameters\n",
    "grid_params = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__max_df': [0.5, 0.75],\n",
    "    'tfidf__min_df': [1, 2],\n",
    "    'tfidf__use_idf': [True, False],\n",
    "    'clf__C': [0.1, 1.0],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__solver': ['liblinear'],\n",
    "}\n",
    "\n",
    "# Randomized Search parameters\n",
    "random_params = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__max_df': uniform(0.5, 0.5),\n",
    "    'tfidf__min_df': randint(1, 3),\n",
    "    'tfidf__use_idf': [True, False],\n",
    "    'clf__C': uniform(0.1, 10.0),\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__solver': ['liblinear'],\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(pipeline, grid_params, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Perform Randomized Search\n",
    "random_search = RandomizedSearchCV(pipeline, random_params, n_iter=100, cv=5, n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
